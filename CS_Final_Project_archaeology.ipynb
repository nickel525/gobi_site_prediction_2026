{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQxY1/7JQixIqyIz+V7xtY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nickel525/gobi_site_prediction_2026/blob/main/CS_Final_Project_archaeology.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import glob\n",
        "\n",
        "paths = sorted(glob.glob(\"/content/drive/MyDrive/gobi_s2_spring_labeled_chips_13x13_shard*.tfrecord.gz\", recursive=True))\n",
        "print(\"Num shards:\", len(paths))\n",
        "print(paths[:3])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtwFfvrpREBx",
        "outputId": "ea9db96d-b8b4-4eed-af07-2b7483c3ed0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Num shards: 8\n",
            "['/content/drive/MyDrive/gobi_s2_spring_labeled_chips_13x13_shard0.tfrecord.gz', '/content/drive/MyDrive/gobi_s2_spring_labeled_chips_13x13_shard1.tfrecord.gz', '/content/drive/MyDrive/gobi_s2_spring_labeled_chips_13x13_shard2.tfrecord.gz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "PATCH = 13\n",
        "PLANE = PATCH * PATCH\n",
        "\n",
        "BANDS = [\"blue\",\"green\",\"red\",\"nir\",\"swir1\",\"swir2\",\"ndvi\",\"ndwi\",\"nbr\"]\n",
        "\n",
        "feature_spec = {b: tf.io.FixedLenFeature([PLANE], tf.float32) for b in BANDS}\n",
        "# scalar float features (length 1)\n",
        "for k in [\"label\",\"row\",\"col\",\"shard\",\"cell_order\"]:\n",
        "    feature_spec[k] = tf.io.FixedLenFeature([1], tf.float32)\n",
        "# bytes features\n",
        "feature_spec[\"cell_id\"] = tf.io.FixedLenFeature([], tf.string)\n",
        "feature_spec[\"system:index\"] = tf.io.FixedLenFeature([], tf.string)\n",
        "\n",
        "def parse_example(x):\n",
        "    ex = tf.io.parse_single_example(x, feature_spec)\n",
        "\n",
        "    # stack planes into (289, 9) then reshape to (17,17,9)\n",
        "    planes = [ex[b] for b in BANDS]                 # each (289,)\n",
        "    stacked = tf.stack(planes, axis=-1)             # (289, 9)\n",
        "    chip = tf.reshape(stacked, [PATCH, PATCH, len(BANDS)])  # (17,17,9)\n",
        "\n",
        "    # label scalar\n",
        "    label = tf.reshape(ex[\"label\"], [])             # scalar\n",
        "    label = tf.cast(label, tf.float32)\n",
        "\n",
        "    # optional metadata (keep for debugging / later mapping)\n",
        "    meta = {\n",
        "        \"row\": tf.cast(tf.reshape(ex[\"row\"], []), tf.int32),\n",
        "        \"col\": tf.cast(tf.reshape(ex[\"col\"], []), tf.int32),\n",
        "        \"shard\": tf.cast(tf.reshape(ex[\"shard\"], []), tf.int32),\n",
        "        \"cell_order\": tf.cast(tf.reshape(ex[\"cell_order\"], []), tf.int32),\n",
        "        \"cell_id\": ex[\"cell_id\"],\n",
        "        \"system_index\": ex[\"system:index\"],\n",
        "    }\n",
        "    return chip, label, meta\n",
        "ds = tf.data.TFRecordDataset(paths, compression_type=\"GZIP\", num_parallel_reads=tf.data.AUTOTUNE)\n",
        "ds = ds.map(parse_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Peek one example\n",
        "chip, y, meta = next(iter(ds.take(1)))\n",
        "print(\"chip:\", chip.shape, chip.dtype)\n",
        "print(\"label:\", y.numpy())\n",
        "print(\"row/col:\", int(meta[\"row\"].numpy()), int(meta[\"col\"].numpy()))\n",
        "print(\"bands:\", BANDS)\n",
        "print(\"min/max per chip:\", float(tf.reduce_min(chip).numpy()), float(tf.reduce_max(chip).numpy()))\n"
      ],
      "metadata": {
        "id": "F0gvdOKFR8lK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c117982-e337-4aa0-e172-837546e6ef1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chip: (13, 13, 9) <dtype: 'float32'>\n",
            "label: 0.0\n",
            "row/col: 1 541\n",
            "bands: ['blue', 'green', 'red', 'nir', 'swir1', 'swir2', 'ndvi', 'ndwi', 'nbr']\n",
            "min/max per chip: -0.276074081659317 0.37489134073257446\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def filter_by_shards(ds, shard_set):\n",
        "    shard_set = tf.constant(list(shard_set), dtype=tf.int32)\n",
        "    def _keep(chip, y, meta):\n",
        "        # meta[\"shard\"] is scalar int32\n",
        "        return tf.reduce_any(tf.equal(meta[\"shard\"], shard_set))\n",
        "    return ds.filter(_keep)\n",
        "\n",
        "# ---- Split by shard ----\n",
        "train_ds = filter_by_shards(ds, {0,1,2,3,4,5})\n",
        "val_ds   = filter_by_shards(ds, {6})\n",
        "test_ds  = filter_by_shards(ds, {7})\n",
        "\n",
        "# ---- Drop metadata (KEEP UNBATCHED) ----\n",
        "def drop_meta(chip, y, meta):\n",
        "    return chip, y\n",
        "\n",
        "train_xy = train_ds.map(drop_meta, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_xy   = val_ds.map(drop_meta,   num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_xy  = test_ds.map(drop_meta,  num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# ---- Augmentation (UNBATCHED) ----\n",
        "def augment(x, y):\n",
        "    x = tf.image.random_flip_left_right(x)\n",
        "    x = tf.image.random_flip_up_down(x)\n",
        "    k = tf.random.uniform([], 0, 4, dtype=tf.int32)\n",
        "    x = tf.image.rot90(x, k)\n",
        "    return x, y\n",
        "\n",
        "train_xy_aug = train_xy.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# ---- Compute mean/std from UNBATCHED training chips (no normalize yet) ----\n",
        "N = 5000\n",
        "xs = []\n",
        "for x, y in train_xy.take(N):   # NOTE: use train_xy (no aug) for stable stats\n",
        "    xs.append(x.numpy())\n",
        "X = np.stack(xs)  # (N, 13, 13, 9)\n",
        "\n",
        "mean = X.reshape(-1, X.shape[-1]).mean(axis=0)\n",
        "std  = X.reshape(-1, X.shape[-1]).std(axis=0) + 1e-6\n",
        "\n",
        "print(\"mean:\", mean)\n",
        "print(\"std :\", std)\n",
        "\n",
        "mean_tf = tf.constant(mean, dtype=tf.float32)\n",
        "std_tf  = tf.constant(std,  dtype=tf.float32)\n",
        "\n",
        "def normalize(x, y):\n",
        "    x = (x - mean_tf) / std_tf\n",
        "    return x, y\n",
        "\n",
        "# ---- Normalize (STILL UNBATCHED) ----\n",
        "train_xy_n_unbatched = train_xy_aug.map(normalize, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_xy_n_unbatched   = val_xy.map(normalize,      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "test_xy_n_unbatched  = test_xy.map(normalize,     num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# ---- Batch EXACTLY ONCE, repeat ONLY train ----\n",
        "BATCH = 64\n",
        "\n",
        "train_xy_n = (train_xy_n_unbatched\n",
        "              .shuffle(5000)\n",
        "              .batch(BATCH, drop_remainder=True)\n",
        "              .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "val_xy_n = (val_xy_n_unbatched\n",
        "            .batch(BATCH)\n",
        "            .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "test_xy_n = (test_xy_n_unbatched\n",
        "             .batch(BATCH)\n",
        "             .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "# ---- Sanity check shapes ----\n",
        "for xb, yb in train_xy_n.take(1):\n",
        "    print(\"TRAIN batch:\", xb.shape, yb.shape)\n",
        "for xb, yb in val_xy_n.take(1):\n",
        "    print(\"VAL batch:\", xb.shape, yb.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwVugGwtSRY4",
        "outputId": "6ed004d6-969b-430d-d5b0-f4279045e512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean: [ 0.11804193  0.1643592   0.2300159   0.28792363  0.372446    0.3405337\n",
            "  0.11277924 -0.2710927  -0.08047115]\n",
            "std : [0.00143058 0.00199728 0.00535583 0.00653893 0.00404383 0.00410531\n",
            " 0.00407734 0.00919885 0.0118668 ]\n",
            "TRAIN batch: (64, 13, 13, 9) (64,)\n",
            "VAL batch: (64, 13, 13, 9) (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos = 0\n",
        "tot = 0\n",
        "for _, y in train_xy_n.unbatch().take(20000):  # sample\n",
        "    pos += int(y.numpy() == 1.0)\n",
        "    tot += 1\n",
        "neg = tot - pos\n",
        "print(\"sampled train pos/neg:\", pos, neg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAq0bCyjSbUj",
        "outputId": "706cd6fd-1eca-411f-abe6-bffeb4e1ee1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sampled train pos/neg: 285 9507\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "PATCH = 13\n",
        "C = len(BANDS)\n",
        "BATCH = 64  # keep in sync with how you batched train_xy_n/val_xy_n\n",
        "\n",
        "# ---- Steps per epoch (robust) ----\n",
        "train_batches = tf.data.experimental.cardinality(train_xy_n).numpy()\n",
        "val_batches   = tf.data.experimental.cardinality(val_xy_n).numpy()\n",
        "\n",
        "# If cardinality is unknown (-2), fall back to counts you measured\n",
        "# train: 9840 examples, val: 1576 examples\n",
        "if train_batches < 0:\n",
        "    train_batches = math.ceil(9840 / BATCH)\n",
        "if val_batches < 0:\n",
        "    val_batches = math.ceil(1576 / BATCH)\n",
        "\n",
        "steps_per_epoch = 153\n",
        "validation_steps = int(val_batches)\n",
        "\n",
        "print(\"steps_per_epoch:\", steps_per_epoch, \"validation_steps:\", validation_steps)\n",
        "\n",
        "# ---- Model ----\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(PATCH, PATCH, C)),\n",
        "    tf.keras.layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
        "    tf.keras.layers.GlobalAveragePooling2D(),\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics = [\n",
        "        tf.keras.metrics.AUC(name=\"auc\"),\n",
        "        tf.keras.metrics.AUC(curve=\"PR\", name=\"prauc\"),\n",
        "        tf.keras.metrics.Precision(name=\"prec_t30\", thresholds=0.30),\n",
        "        tf.keras.metrics.Recall(name=\"rec_t30\", thresholds=0.30),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ---- Class weights (moderate; your pos rate ~3%) ----\n",
        "# Start with 10â€“15x instead of ~33x to avoid \"trigger-happy\" behavior.\n",
        "class_weight = {0: 1.0, 1: 6.0}\n",
        "\n",
        "# ---- Callbacks ----\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"best_cnn.keras\",\n",
        "        monitor=\"val_prauc\",\n",
        "        mode=\"max\",\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_prauc\",\n",
        "        mode=\"max\",\n",
        "        patience=10,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor=\"val_prauc\",\n",
        "        mode=\"max\",\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-5,\n",
        "        verbose=1\n",
        "    ),\n",
        "]\n",
        "\n",
        "# ---- Train ----\n",
        "history = model.fit(\n",
        "    train_xy_n,                 # IMPORTANT: prevents running out of data\n",
        "    validation_data=val_xy_n,\n",
        "    epochs=50,\n",
        "    validation_steps=validation_steps,\n",
        "    class_weight=class_weight,\n",
        "    callbacks=callbacks,\n",
        "    verbose=2\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuJofmpeSkWA",
        "outputId": "a3013361-45f8-4714-d24f-2ffd0eb04be3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "steps_per_epoch: 153 validation_steps: 25\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/epoch_iterator.py:160: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: val_prauc improved from -inf to 0.14923, saving model to best_cnn.keras\n",
            "153/153 - 16s - 101ms/step - auc: 0.5820 - loss: 0.5292 - prauc: 0.0431 - prec_t30: 0.0400 - rec_t30: 0.3227 - val_auc: 0.5843 - val_loss: 0.2573 - val_prauc: 0.1492 - val_prec_t30: 0.1060 - val_rec_t30: 0.2963 - learning_rate: 1.0000e-03\n",
            "Epoch 2/50\n",
            "\n",
            "Epoch 2: val_prauc did not improve from 0.14923\n",
            "153/153 - 14s - 91ms/step - auc: 0.5822 - loss: 0.4957 - prauc: 0.0419 - prec_t30: 0.0539 - rec_t30: 0.2000 - val_auc: 0.6266 - val_loss: 0.2083 - val_prauc: 0.1048 - val_prec_t30: 0.0000e+00 - val_rec_t30: 0.0000e+00 - learning_rate: 1.0000e-03\n",
            "Epoch 3/50\n",
            "\n",
            "Epoch 3: val_prauc did not improve from 0.14923\n",
            "153/153 - 18s - 120ms/step - auc: 0.6308 - loss: 0.4808 - prauc: 0.0495 - prec_t30: 0.0548 - rec_t30: 0.1789 - val_auc: 0.6184 - val_loss: 0.2529 - val_prauc: 0.0493 - val_prec_t30: 0.0417 - val_rec_t30: 0.0556 - learning_rate: 1.0000e-03\n",
            "Epoch 4/50\n",
            "\n",
            "Epoch 4: val_prauc did not improve from 0.14923\n",
            "\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "153/153 - 52s - 341ms/step - auc: 0.6533 - loss: 0.4725 - prauc: 0.0512 - prec_t30: 0.0673 - rec_t30: 0.2125 - val_auc: 0.6522 - val_loss: 0.2022 - val_prauc: 0.0916 - val_prec_t30: 0.1622 - val_rec_t30: 0.2222 - learning_rate: 1.0000e-03\n",
            "Epoch 5/50\n",
            "\n",
            "Epoch 5: val_prauc did not improve from 0.14923\n",
            "153/153 - 12s - 78ms/step - auc: 0.6902 - loss: 0.4537 - prauc: 0.0616 - prec_t30: 0.0747 - rec_t30: 0.2028 - val_auc: 0.6784 - val_loss: 0.2030 - val_prauc: 0.1032 - val_prec_t30: 0.1552 - val_rec_t30: 0.1667 - learning_rate: 5.0000e-04\n",
            "Epoch 6/50\n",
            "\n",
            "Epoch 6: val_prauc did not improve from 0.14923\n",
            "153/153 - 12s - 76ms/step - auc: 0.7100 - loss: 0.4401 - prauc: 0.0733 - prec_t30: 0.0910 - rec_t30: 0.2351 - val_auc: 0.6756 - val_loss: 0.2157 - val_prauc: 0.0733 - val_prec_t30: 0.1273 - val_rec_t30: 0.2593 - learning_rate: 5.0000e-04\n",
            "Epoch 7/50\n",
            "\n",
            "Epoch 7: val_prauc did not improve from 0.14923\n",
            "\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "153/153 - 15s - 95ms/step - auc: 0.7102 - loss: 0.4424 - prauc: 0.0859 - prec_t30: 0.1021 - rec_t30: 0.2578 - val_auc: 0.6895 - val_loss: 0.2143 - val_prauc: 0.1054 - val_prec_t30: 0.1818 - val_rec_t30: 0.2593 - learning_rate: 5.0000e-04\n",
            "Epoch 8/50\n",
            "\n",
            "Epoch 8: val_prauc improved from 0.14923 to 0.18497, saving model to best_cnn.keras\n",
            "153/153 - 10s - 66ms/step - auc: 0.7105 - loss: 0.4414 - prauc: 0.0693 - prec_t30: 0.0830 - rec_t30: 0.2552 - val_auc: 0.7117 - val_loss: 0.2183 - val_prauc: 0.1850 - val_prec_t30: 0.1731 - val_rec_t30: 0.3333 - learning_rate: 2.5000e-04\n",
            "Epoch 9/50\n",
            "\n",
            "Epoch 9: val_prauc did not improve from 0.18497\n",
            "153/153 - 13s - 87ms/step - auc: 0.7361 - loss: 0.4298 - prauc: 0.0865 - prec_t30: 0.0948 - rec_t30: 0.2807 - val_auc: 0.7122 - val_loss: 0.2070 - val_prauc: 0.1658 - val_prec_t30: 0.2078 - val_rec_t30: 0.2963 - learning_rate: 2.5000e-04\n",
            "Epoch 10/50\n",
            "\n",
            "Epoch 10: val_prauc did not improve from 0.18497\n",
            "153/153 - 13s - 82ms/step - auc: 0.7406 - loss: 0.4261 - prauc: 0.0863 - prec_t30: 0.0976 - rec_t30: 0.3077 - val_auc: 0.7262 - val_loss: 0.2026 - val_prauc: 0.1696 - val_prec_t30: 0.2500 - val_rec_t30: 0.2963 - learning_rate: 2.5000e-04\n",
            "Epoch 11/50\n",
            "\n",
            "Epoch 11: val_prauc did not improve from 0.18497\n",
            "\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "153/153 - 21s - 138ms/step - auc: 0.7412 - loss: 0.4273 - prauc: 0.1035 - prec_t30: 0.1144 - rec_t30: 0.2993 - val_auc: 0.7372 - val_loss: 0.1959 - val_prauc: 0.1664 - val_prec_t30: 0.2388 - val_rec_t30: 0.2963 - learning_rate: 2.5000e-04\n",
            "Epoch 12/50\n",
            "\n",
            "Epoch 12: val_prauc did not improve from 0.18497\n",
            "153/153 - 11s - 72ms/step - auc: 0.7444 - loss: 0.4254 - prauc: 0.1054 - prec_t30: 0.1178 - rec_t30: 0.3217 - val_auc: 0.7587 - val_loss: 0.2056 - val_prauc: 0.1565 - val_prec_t30: 0.3061 - val_rec_t30: 0.2778 - learning_rate: 1.2500e-04\n",
            "Epoch 13/50\n",
            "\n",
            "Epoch 13: val_prauc did not improve from 0.18497\n",
            "153/153 - 13s - 82ms/step - auc: 0.7463 - loss: 0.4210 - prauc: 0.1045 - prec_t30: 0.1176 - rec_t30: 0.3042 - val_auc: 0.7639 - val_loss: 0.2016 - val_prauc: 0.1617 - val_prec_t30: 0.2778 - val_rec_t30: 0.2778 - learning_rate: 1.2500e-04\n",
            "Epoch 14/50\n",
            "\n",
            "Epoch 14: val_prauc improved from 0.18497 to 0.19911, saving model to best_cnn.keras\n",
            "153/153 - 71s - 467ms/step - auc: 0.7519 - loss: 0.4205 - prauc: 0.0974 - prec_t30: 0.1083 - rec_t30: 0.2822 - val_auc: 0.7713 - val_loss: 0.1994 - val_prauc: 0.1991 - val_prec_t30: 0.2462 - val_rec_t30: 0.2963 - learning_rate: 1.2500e-04\n",
            "Epoch 15/50\n",
            "\n",
            "Epoch 15: val_prauc did not improve from 0.19911\n",
            "153/153 - 11s - 70ms/step - auc: 0.7506 - loss: 0.4185 - prauc: 0.1032 - prec_t30: 0.1061 - rec_t30: 0.2692 - val_auc: 0.7698 - val_loss: 0.2117 - val_prauc: 0.1826 - val_prec_t30: 0.2500 - val_rec_t30: 0.2963 - learning_rate: 1.2500e-04\n",
            "Epoch 16/50\n",
            "\n",
            "Epoch 16: val_prauc did not improve from 0.19911\n",
            "153/153 - 12s - 79ms/step - auc: 0.7482 - loss: 0.4188 - prauc: 0.1050 - prec_t30: 0.1086 - rec_t30: 0.2997 - val_auc: 0.7684 - val_loss: 0.2038 - val_prauc: 0.1890 - val_prec_t30: 0.2388 - val_rec_t30: 0.2963 - learning_rate: 1.2500e-04\n",
            "Epoch 17/50\n",
            "\n",
            "Epoch 17: val_prauc did not improve from 0.19911\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "153/153 - 12s - 79ms/step - auc: 0.7538 - loss: 0.4204 - prauc: 0.1015 - prec_t30: 0.1199 - rec_t30: 0.2787 - val_auc: 0.7865 - val_loss: 0.2057 - val_prauc: 0.1904 - val_prec_t30: 0.2759 - val_rec_t30: 0.2963 - learning_rate: 1.2500e-04\n",
            "Epoch 18/50\n",
            "\n",
            "Epoch 18: val_prauc did not improve from 0.19911\n",
            "153/153 - 12s - 79ms/step - auc: 0.7552 - loss: 0.4147 - prauc: 0.1012 - prec_t30: 0.1084 - rec_t30: 0.2912 - val_auc: 0.7868 - val_loss: 0.1985 - val_prauc: 0.1877 - val_prec_t30: 0.2857 - val_rec_t30: 0.2963 - learning_rate: 6.2500e-05\n",
            "Epoch 19/50\n",
            "\n",
            "Epoch 19: val_prauc did not improve from 0.19911\n",
            "153/153 - 12s - 75ms/step - auc: 0.7619 - loss: 0.4117 - prauc: 0.1008 - prec_t30: 0.0980 - rec_t30: 0.2782 - val_auc: 0.7793 - val_loss: 0.2020 - val_prauc: 0.1707 - val_prec_t30: 0.2388 - val_rec_t30: 0.2963 - learning_rate: 6.2500e-05\n",
            "Epoch 20/50\n",
            "\n",
            "Epoch 20: val_prauc did not improve from 0.19911\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "153/153 - 10s - 68ms/step - auc: 0.7654 - loss: 0.4119 - prauc: 0.1071 - prec_t30: 0.1306 - rec_t30: 0.2867 - val_auc: 0.7859 - val_loss: 0.2007 - val_prauc: 0.1816 - val_prec_t30: 0.2581 - val_rec_t30: 0.2963 - learning_rate: 6.2500e-05\n",
            "Epoch 21/50\n",
            "\n",
            "Epoch 21: val_prauc did not improve from 0.19911\n",
            "153/153 - 50s - 329ms/step - auc: 0.7652 - loss: 0.4101 - prauc: 0.1080 - prec_t30: 0.1169 - rec_t30: 0.2982 - val_auc: 0.7874 - val_loss: 0.1997 - val_prauc: 0.1846 - val_prec_t30: 0.2540 - val_rec_t30: 0.2963 - learning_rate: 3.1250e-05\n",
            "Epoch 22/50\n",
            "\n",
            "Epoch 22: val_prauc improved from 0.19911 to 0.19982, saving model to best_cnn.keras\n",
            "153/153 - 13s - 82ms/step - auc: 0.7652 - loss: 0.4100 - prauc: 0.1099 - prec_t30: 0.1192 - rec_t30: 0.3077 - val_auc: 0.7872 - val_loss: 0.1982 - val_prauc: 0.1998 - val_prec_t30: 0.2540 - val_rec_t30: 0.2963 - learning_rate: 3.1250e-05\n",
            "Epoch 23/50\n",
            "\n",
            "Epoch 23: val_prauc did not improve from 0.19982\n",
            "153/153 - 21s - 136ms/step - auc: 0.7617 - loss: 0.4086 - prauc: 0.1016 - prec_t30: 0.1252 - rec_t30: 0.2982 - val_auc: 0.7905 - val_loss: 0.1984 - val_prauc: 0.1974 - val_prec_t30: 0.2540 - val_rec_t30: 0.2963 - learning_rate: 3.1250e-05\n",
            "Epoch 24/50\n",
            "\n",
            "Epoch 24: val_prauc did not improve from 0.19982\n",
            "153/153 - 11s - 75ms/step - auc: 0.7631 - loss: 0.4106 - prauc: 0.1243 - prec_t30: 0.1190 - rec_t30: 0.3053 - val_auc: 0.7931 - val_loss: 0.1968 - val_prauc: 0.1854 - val_prec_t30: 0.2759 - val_rec_t30: 0.2963 - learning_rate: 3.1250e-05\n",
            "Epoch 25/50\n",
            "\n",
            "Epoch 25: val_prauc did not improve from 0.19982\n",
            "\n",
            "Epoch 25: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "153/153 - 22s - 142ms/step - auc: 0.7612 - loss: 0.4124 - prauc: 0.1134 - prec_t30: 0.1255 - rec_t30: 0.3042 - val_auc: 0.7923 - val_loss: 0.1992 - val_prauc: 0.1877 - val_prec_t30: 0.2540 - val_rec_t30: 0.2963 - learning_rate: 3.1250e-05\n",
            "Epoch 26/50\n",
            "\n",
            "Epoch 26: val_prauc did not improve from 0.19982\n",
            "153/153 - 19s - 127ms/step - auc: 0.7647 - loss: 0.4073 - prauc: 0.1113 - prec_t30: 0.1185 - rec_t30: 0.2887 - val_auc: 0.7938 - val_loss: 0.1966 - val_prauc: 0.1913 - val_prec_t30: 0.2667 - val_rec_t30: 0.2963 - learning_rate: 1.5625e-05\n",
            "Epoch 27/50\n",
            "\n",
            "Epoch 27: val_prauc did not improve from 0.19982\n",
            "153/153 - 13s - 82ms/step - auc: 0.7689 - loss: 0.4086 - prauc: 0.1130 - prec_t30: 0.1159 - rec_t30: 0.2787 - val_auc: 0.7929 - val_loss: 0.1976 - val_prauc: 0.1962 - val_prec_t30: 0.2623 - val_rec_t30: 0.2963 - learning_rate: 1.5625e-05\n",
            "Epoch 28/50\n",
            "\n",
            "Epoch 28: val_prauc did not improve from 0.19982\n",
            "\n",
            "Epoch 28: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "153/153 - 12s - 81ms/step - auc: 0.7756 - loss: 0.4026 - prauc: 0.1091 - prec_t30: 0.1261 - rec_t30: 0.3088 - val_auc: 0.7922 - val_loss: 0.1965 - val_prauc: 0.1985 - val_prec_t30: 0.2712 - val_rec_t30: 0.2963 - learning_rate: 1.5625e-05\n",
            "Epoch 29/50\n",
            "\n",
            "Epoch 29: val_prauc did not improve from 0.19982\n",
            "153/153 - 12s - 77ms/step - auc: 0.7693 - loss: 0.4087 - prauc: 0.1118 - prec_t30: 0.1268 - rec_t30: 0.3147 - val_auc: 0.7943 - val_loss: 0.1975 - val_prauc: 0.1897 - val_prec_t30: 0.2667 - val_rec_t30: 0.2963 - learning_rate: 1.0000e-05\n",
            "Epoch 30/50\n",
            "\n",
            "Epoch 30: val_prauc did not improve from 0.19982\n",
            "153/153 - 10s - 68ms/step - auc: 0.7654 - loss: 0.4115 - prauc: 0.1063 - prec_t30: 0.1207 - rec_t30: 0.2937 - val_auc: 0.7950 - val_loss: 0.1975 - val_prauc: 0.1827 - val_prec_t30: 0.2667 - val_rec_t30: 0.2963 - learning_rate: 1.0000e-05\n",
            "Epoch 31/50\n",
            "\n",
            "Epoch 31: val_prauc did not improve from 0.19982\n",
            "153/153 - 21s - 134ms/step - auc: 0.7641 - loss: 0.4083 - prauc: 0.1040 - prec_t30: 0.1145 - rec_t30: 0.2877 - val_auc: 0.7940 - val_loss: 0.1971 - val_prauc: 0.1913 - val_prec_t30: 0.2712 - val_rec_t30: 0.2963 - learning_rate: 1.0000e-05\n",
            "Epoch 32/50\n",
            "\n",
            "Epoch 32: val_prauc did not improve from 0.19982\n",
            "153/153 - 12s - 76ms/step - auc: 0.7700 - loss: 0.4085 - prauc: 0.1251 - prec_t30: 0.1110 - rec_t30: 0.2787 - val_auc: 0.7954 - val_loss: 0.1966 - val_prauc: 0.1848 - val_prec_t30: 0.2712 - val_rec_t30: 0.2963 - learning_rate: 1.0000e-05\n",
            "Epoch 32: early stopping\n",
            "Restoring model weights from the end of the best epoch: 22.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = \"/content/drive/MyDrive/gobi_cnn_model.keras\"\n",
        "model.save(MODEL_DIR)\n",
        "\n",
        "print(\"Model saved to:\", MODEL_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Up7waUrLVLHJ",
        "outputId": "d09847a7-8160-4ecd-df0c-78453d47960f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/MyDrive/gobi_cnn_model.keras\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "\n",
        "norm = {\n",
        "    \"bands\": BANDS,\n",
        "    \"mean\": mean.tolist(),\n",
        "    \"std\": std.tolist(),\n",
        "}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/gobi_cnn_model_norm.json\", \"w\") as f:\n",
        "    json.dump(norm, f, indent=2)\n",
        "\n",
        "print(\"Normalization saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34H0vBUabAo2",
        "outputId": "e5b5e5f6-f01d-4a85-d813-7a9137f64e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import json\n",
        "\n",
        "model = tf.keras.models.load_model(\"/content/drive/MyDrive/gobi_cnn_model.keras\")\n",
        "\n",
        "with open(\"/content/drive/MyDrive/gobi_cnn_model_norm.json\") as f:\n",
        "    norm = json.load(f)\n",
        "\n",
        "mean = tf.constant(norm[\"mean\"], dtype=tf.float32)\n",
        "std  = tf.constant(norm[\"std\"], dtype=tf.float32)\n",
        "\n",
        "print(\"Reload successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mo5mhtwbIGA",
        "outputId": "8d7fb51a-fe65-4c68-d5ee-1c29b7d2c3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reload successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_preds_and_labels(ds):\n",
        "    ys = []\n",
        "    ps = []\n",
        "    for x_batch, y_batch in ds:\n",
        "        p_batch = model.predict(x_batch, verbose=0).reshape(-1)\n",
        "        ps.append(p_batch)\n",
        "        ys.append(y_batch.numpy().reshape(-1))\n",
        "    y = np.concatenate(ys).astype(np.int32)\n",
        "    p = np.concatenate(ps).astype(np.float32)\n",
        "    return y, p\n",
        "\n",
        "y_true, p_site = get_preds_and_labels(test_xy_n)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def confusion_at_threshold(y_true, p_site, t):\n",
        "    yhat = (p_site >= t).astype(int)\n",
        "    TP = int(((yhat==1) & (y_true==1)).sum())\n",
        "    FP = int(((yhat==1) & (y_true==0)).sum())\n",
        "    TN = int(((yhat==0) & (y_true==0)).sum())\n",
        "    FN = int(((yhat==0) & (y_true==1)).sum())\n",
        "    prec = TP/(TP+FP+1e-9)\n",
        "    rec  = TP/(TP+FN+1e-9)\n",
        "    acc  = (TP+TN)/(TP+FP+TN+FN)\n",
        "    f1   = 2*prec*rec/(prec+rec+1e-9)\n",
        "    return TP,FP,TN,FN,acc,prec,rec,f1\n",
        "\n",
        "ts = np.linspace(0,1,401)\n",
        "rows = []\n",
        "for t in ts:\n",
        "    rows.append((t,)+confusion_at_threshold(y_true, p_site, t))\n",
        "rows = np.array(rows, dtype=float)\n",
        "\n",
        "# best F1\n",
        "best_f1 = rows[np.argmax(rows[:,-1])]\n",
        "print(\"Best F1:\", best_f1)\n",
        "\n",
        "# threshold that keeps recall >= 0.60 and minimizes FP\n",
        "rows_recall = rows[rows[:,-2] >= 0.60]  # recall column\n",
        "best_fp = rows_recall[np.argmin(rows_recall[:,2])]  # FP column\n",
        "print(\"Min FP with recall>=0.60:\", best_fp)\n",
        "\n",
        "# show a few candidate thresholds around where FP drops\n",
        "for t in [0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "    out = confusion_at_threshold(y_true, p_site, t)\n",
        "    print(t, out)\n",
        "\n",
        "print(\"p min/max:\", p_site.min(), p_site.max())\n",
        "print(\"p quantiles:\", np.quantile(p_site, [0, .01, .05, .1, .5, .9, .95, .99, 1.0]))\n",
        "\n",
        "test_metrics = model.evaluate(test_xy_n, verbose=2)\n",
        "print(dict(zip(model.metrics_names, test_metrics)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5gmYuIUSuQH",
        "outputId": "12f80bb3-890c-4d77-b3e2-a3fdd51bb4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1: [3.20000000e-01 1.60000000e+01 5.00000000e+01 1.54400000e+03\n",
            " 4.10000000e+01 9.44881890e-01 2.42424242e-01 2.80701754e-01\n",
            " 2.60162601e-01]\n",
            "Min FP with recall>=0.60: [2.02500000e-01 3.50000000e+01 3.22000000e+02 1.27200000e+03\n",
            " 2.20000000e+01 7.91641429e-01 9.80392157e-02 6.14035088e-01\n",
            " 1.69082125e-01]\n",
            "0.1 (57, 1001, 593, 0, 0.3937007874015748, 0.05387523629484511, 0.9999999999824563, 0.10224215236916888)\n",
            "0.2 (36, 329, 1265, 21, 0.788007268322229, 0.09863013698603115, 0.6315789473573408, 0.17061611350961345)\n",
            "0.25 (28, 230, 1364, 29, 0.8431253785584494, 0.1085271317825251, 0.4912280701668206, 0.1777777774802318)\n",
            "0.3 (16, 53, 1541, 41, 0.9430648092065415, 0.23188405796765385, 0.28070175438104034, 0.2539682534687579)\n",
            "0.4 (12, 43, 1551, 45, 0.9466989703210176, 0.21818181817785126, 0.21052631578578027, 0.21428571378204722)\n",
            "0.5 (0, 0, 1594, 57, 0.9654754694124773, 0.0, 0.0, 0.0)\n",
            "0.6 (0, 0, 1594, 57, 0.9654754694124773, 0.0, 0.0, 0.0)\n",
            "0.7 (0, 0, 1594, 57, 0.9654754694124773, 0.0, 0.0, 0.0)\n",
            "0.8 (0, 0, 1594, 57, 0.9654754694124773, 0.0, 0.0, 0.0)\n",
            "0.9 (0, 0, 1594, 57, 0.9654754694124773, 0.0, 0.0, 0.0)\n",
            "p min/max: 5.98619e-06 0.49788693\n",
            "p quantiles: [5.98618999e-06 8.28512930e-06 1.12872495e-03 3.75066628e-03\n",
            " 1.36800066e-01 2.79704481e-01 2.86762252e-01 4.94111747e-01\n",
            " 4.97886926e-01]\n",
            "26/26 - 4s - 166ms/step - auc: 0.8059 - loss: 0.1922 - prauc: 0.1379 - prec_t30: 0.2319 - rec_t30: 0.2807\n",
            "{'loss': 0.1922100931406021, 'compile_metrics': 0.8059114217758179}\n"
          ]
        }
      ]
    }
  ]
}